#!/usr/bin/env python
"""
WANãƒ¢ãƒ‡ãƒ«ã§å®Ÿéš›ã«å‘¼ã³å‡ºã•ã‚Œã‚‹é–¢æ•°ã‚’è¿½è·¡
"""
import torch
import sys
import os

def trace_all_wan_functions():
    """WANãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å…¨é–¢æ•°ã«ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¿½åŠ """
    
    try:
        import wan.modules.attention as wan_attn
        
        print("WANã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å…¨é–¢æ•°ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ä¸­...")
        print("="*50)
        
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†…ã®å…¨ã¦ã®é–¢æ•°ã‚’å–å¾—
        all_functions = []
        for attr_name in dir(wan_attn):
            attr = getattr(wan_attn, attr_name)
            if callable(attr) and not attr_name.startswith('_'):
                all_functions.append((attr_name, attr))
        
        print(f"ç™ºè¦‹ã—ãŸé–¢æ•°: {[name for name, _ in all_functions]}")
        
        # å„é–¢æ•°ã«ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¿½åŠ 
        original_functions = {}
        for func_name, func in all_functions:
            try:
                # å…ƒã®é–¢æ•°ã‚’ä¿å­˜
                original_functions[func_name] = func
                
                # ãƒˆãƒ¬ãƒ¼ã‚¹ä»˜ããƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½œæˆ
                def make_tracer(name, original_func):
                    def traced_func(*args, **kwargs):
                        print(f"ğŸ” WAN function called: {name}")
                        if args:
                            print(f"  â†’ args count: {len(args)}")
                            for i, arg in enumerate(args[:3]):  # æœ€åˆã®3ã¤ã®å¼•æ•°ã®ã¿è¡¨ç¤º
                                if hasattr(arg, 'shape'):
                                    print(f"    arg[{i}] shape: {arg.shape}")
                                else:
                                    print(f"    arg[{i}] type: {type(arg)}")
                        if kwargs:
                            print(f"  â†’ kwargs: {list(kwargs.keys())}")
                        
                        result = original_func(*args, **kwargs)
                        
                        if hasattr(result, 'shape'):
                            print(f"  â†’ result shape: {result.shape}")
                        
                        return result
                    return traced_func
                
                # é–¢æ•°ã‚’ç½®ãæ›ãˆ
                setattr(wan_attn, func_name, make_tracer(func_name, func))
                print(f"âœ“ Traced: {func_name}")
                
            except Exception as e:
                print(f"âœ— Failed to trace {func_name}: {e}")
        
        return original_functions
        
    except ImportError as e:
        print(f"Cannot import WAN attention module: {e}")
        return {}
    except Exception as e:
        print(f"Error setting up traces: {e}")
        return {}

def trace_torch_attention():
    """PyTorchã®æ¨™æº–ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é–¢æ•°ã‚‚ãƒˆãƒ¬ãƒ¼ã‚¹"""
    
    print("\nPyTorchã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é–¢æ•°ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ä¸­...")
    print("="*40)
    
    try:
        # scaled_dot_product_attention ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹
        original_sdpa = torch.nn.functional.scaled_dot_product_attention
        
        def traced_sdpa(*args, **kwargs):
            print("ğŸ” PyTorch scaled_dot_product_attention called")
            if args:
                print(f"  â†’ q shape: {args[0].shape}")
                print(f"  â†’ k shape: {args[1].shape}")
                print(f"  â†’ v shape: {args[2].shape}")
            
            result = original_sdpa(*args, **kwargs)
            print(f"  â†’ output shape: {result.shape}")
            return result
        
        torch.nn.functional.scaled_dot_product_attention = traced_sdpa
        print("âœ“ Traced: torch.nn.functional.scaled_dot_product_attention")
        
    except Exception as e:
        print(f"Failed to trace PyTorch attention: {e}")

def trace_flash_attn():
    """Flash Attentionã®é–¢æ•°ã‚‚ãƒˆãƒ¬ãƒ¼ã‚¹"""
    
    print("\nFlash Attentioné–¢æ•°ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ä¸­...")
    print("="*30)
    
    try:
        from flash_attn import flash_attn_func
        import flash_attn
        
        # flash_attn_func ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹
        original_flash_func = flash_attn_func
        
        def traced_flash_func(*args, **kwargs):
            print("ğŸš€ flash_attn_func called!")
            if args:
                print(f"  â†’ q shape: {args[0].shape}")
                print(f"  â†’ k shape: {args[1].shape}")
                print(f"  â†’ v shape: {args[2].shape}")
            
            result = original_flash_func(*args, **kwargs)
            print(f"  â†’ output shape: {result.shape}")
            return result
        
        # ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒ
        flash_attn.flash_attn_func = traced_flash_func
        
        print("âœ“ Traced: flash_attn.flash_attn_func")
        
    except ImportError:
        print("âœ— Flash Attention not available")
    except Exception as e:
        print(f"Failed to trace Flash Attention: {e}")

def setup_comprehensive_tracing():
    """åŒ…æ‹¬çš„ãªãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¨­å®š"""
    
    print("åŒ…æ‹¬çš„ãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®šä¸­...")
    print("="*60)
    
    # WANã®é–¢æ•°ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹
    wan_originals = trace_all_wan_functions()
    
    # PyTorchã®é–¢æ•°ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹
    trace_torch_attention()
    
    # Flash Attentionã‚’ãƒˆãƒ¬ãƒ¼ã‚¹
    trace_flash_attn()
    
    print(f"\nâœ“ ãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®šå®Œäº† ({len(wan_originals)} WAN functions traced)")
    print("ã“ã‚Œã§å®Ÿéš›ã«å‘¼ã³å‡ºã•ã‚Œã‚‹é–¢æ•°ãŒã™ã¹ã¦è¡¨ç¤ºã•ã‚Œã¾ã™")
    
    return wan_originals

def inspect_wan_model_structure():
    """WANãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã‚’è©³ã—ãèª¿æŸ»"""
    
    print("\nWANãƒ¢ãƒ‡ãƒ«æ§‹é€ èª¿æŸ»...")
    print("="*30)
    
    try:
        # WANã®AttentionBlockã‚¯ãƒ©ã‚¹ã‚’ç›´æ¥ç¢ºèª
        import wan.modules.model as wan_model
        
        print("wan.modules.model ã§åˆ©ç”¨å¯èƒ½ãªã‚¯ãƒ©ã‚¹:")
        model_classes = [attr for attr in dir(wan_model) if not attr.startswith('_') and isinstance(getattr(wan_model, attr), type)]
        for cls_name in model_classes:
            cls = getattr(wan_model, cls_name)
            print(f"  - {cls_name}: {cls}")
            
            # Attentionã«é–¢é€£ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¢ã™
            attention_methods = [method for method in dir(cls) if 'attn' in method.lower() and not method.startswith('_')]
            if attention_methods:
                print(f"    Attention methods: {attention_methods}")
        
        # attention.py ã®è©³ç´°ã‚’èª¿æŸ»
        import wan.modules.attention as wan_attn
        print(f"\nwan.modules.attention ã§åˆ©ç”¨å¯èƒ½ãªè¦ç´ :")
        for attr in dir(wan_attn):
            if not attr.startswith('_'):
                obj = getattr(wan_attn, attr)
                print(f"  - {attr}: {type(obj)}")
        
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«æ§‹é€ èª¿æŸ»ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    # æ§‹é€ èª¿æŸ»
    inspect_wan_model_structure()
    
    # åŒ…æ‹¬çš„ãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®š
    setup_comprehensive_tracing()
    
    print("\n" + "="*60)
    print("ãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®šå®Œäº† - å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‘¼ã³å‡ºã—ã‚’ç¢ºèªã§ãã¾ã™")
    print("="*60)