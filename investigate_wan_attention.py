#!/usr/bin/env python
"""
WANãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‘¼ã³å‡ºã—ã‚’èª¿æŸ»
"""
import torch
import sys
import os

def hook_wan_attention():
    """WANã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é–¢æ•°ã«ãƒ•ãƒƒã‚¯ã‚’è¿½åŠ ã—ã¦å®Ÿéš›ã®ä½¿ç”¨ã‚’ç›£è¦–"""
    
    try:
        import wan.modules.attention as wan_attn
        
        # å…ƒã®é–¢æ•°ã‚’ä¿å­˜
        original_functions = {}
        
        # flash_attentioné–¢æ•°ã‚’ãƒ•ãƒƒã‚¯
        if hasattr(wan_attn, 'flash_attention'):
            original_functions['flash_attention'] = wan_attn.flash_attention
            
            def hooked_flash_attention(*args, **kwargs):
                print(f"ğŸ” flash_attention called with args: {len(args)}, kwargs: {list(kwargs.keys())}")
                if len(args) >= 3:
                    q, k, v = args[0], args[1], args[2]
                    print(f"  â†’ q.shape: {q.shape}, k.shape: {k.shape}, v.shape: {v.shape}")
                return original_functions['flash_attention'](*args, **kwargs)
            
            wan_attn.flash_attention = hooked_flash_attention
            print("âœ“ Hooked wan.modules.attention.flash_attention")
        
        # ãã®ä»–ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é–¢æ•°ã‚‚æ¢ã™
        attention_functions = [attr for attr in dir(wan_attn) if 'attention' in attr.lower() and callable(getattr(wan_attn, attr))]
        print(f"Available attention functions: {attention_functions}")
        
        for func_name in attention_functions:
            if func_name != 'flash_attention':
                func = getattr(wan_attn, func_name)
                original_functions[func_name] = func
                
                def make_hooked_func(name, original_func):
                    def hooked_func(*args, **kwargs):
                        print(f"ğŸ” {name} called")
                        return original_func(*args, **kwargs)
                    return hooked_func
                
                setattr(wan_attn, func_name, make_hooked_func(func_name, func))
                print(f"âœ“ Hooked {func_name}")
        
        return original_functions
        
    except Exception as e:
        print(f"Error hooking WAN attention: {e}")
        return {}

def test_wan_model_attention():
    """WANãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿéš›ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä½¿ç”¨ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("WANãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä½¿ç”¨ãƒ†ã‚¹ãƒˆ")
    print("="*40)
    
    # ãƒ•ãƒƒã‚¯ã‚’è¨­å®š
    hooks = hook_wan_attention()
    
    try:
        from vace.models.wan import OptimizedWanVace
        from vace.models.wan.configs import WAN_CONFIGS
        
        config = WAN_CONFIGS["vace-1.3B"]
        
        # å®Ÿéš›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ãªã®ã§ã€ãƒ€ãƒŸãƒ¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        print("\nãƒ€ãƒŸãƒ¼ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ“ä½œã‚’ãƒ†ã‚¹ãƒˆ...")
        
        # WANã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é–¢æ•°ã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦ãƒ†ã‚¹ãƒˆ
        import wan.modules.attention as wan_attn
        
        if hasattr(wan_attn, 'flash_attention'):
            print("flash_attentioné–¢æ•°ã‚’ç›´æ¥å‘¼ã³å‡ºã—...")
            
            # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            
            # WANã®å…¸å‹çš„ãªå½¢çŠ¶
            batch, heads, seq_len, dim = 1, 16, 1024, 64
            
            q = torch.randn(batch, heads, seq_len, dim, device=device, dtype=torch.half)
            k = torch.randn(batch, heads, seq_len, dim, device=device, dtype=torch.half)
            v = torch.randn(batch, heads, seq_len, dim, device=device, dtype=torch.half)
            k_lens = torch.tensor([seq_len], device=device)
            
            try:
                result = wan_attn.flash_attention(q, k, v, k_lens)
                print(f"âœ“ flash_attention successful, output shape: {result.shape}")
            except Exception as e:
                print(f"âœ— flash_attention failed: {e}")
        
    except Exception as e:
        print(f"Model test error: {e}")
    
    # ãƒ•ãƒƒã‚¯ã‚’å…ƒã«æˆ»ã™
    try:
        import wan.modules.attention as wan_attn
        for func_name, original_func in hooks.items():
            setattr(wan_attn, func_name, original_func)
        print(f"\nâœ“ Restored {len(hooks)} hooked functions")
    except:
        pass

def analyze_wan_source():
    """WANã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’åˆ†æã—ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä½¿ç”¨ç®‡æ‰€ã‚’ç‰¹å®š"""
    
    print("\nWANã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰åˆ†æ")
    print("="*30)
    
    try:
        import wan.modules.attention as wan_attn
        import inspect
        
        # flash_attentioné–¢æ•°ã®ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª
        if hasattr(wan_attn, 'flash_attention'):
            try:
                source = inspect.getsource(wan_attn.flash_attention)
                print("flash_attention function source:")
                print("-" * 20)
                print(source[:500] + "..." if len(source) > 500 else source)
            except:
                print("Cannot get flash_attention source")
        
        # WanAttentionBlockã®åˆ†æ
        try:
            from wan.modules.attention import WanAttentionBlock
            attention_block_source = inspect.getsource(WanAttentionBlock)
            
            # flash_attentionã®å‘¼ã³å‡ºã—ç®‡æ‰€ã‚’æ¢ã™
            if 'flash_attention' in attention_block_source:
                print("\nâœ“ WanAttentionBlock uses flash_attention")
                
                # å‘¼ã³å‡ºã—ç®‡æ‰€ã®å‘¨è¾ºã‚’è¡¨ç¤º
                lines = attention_block_source.split('\n')
                for i, line in enumerate(lines):
                    if 'flash_attention' in line:
                        start = max(0, i-2)
                        end = min(len(lines), i+3)
                        print(f"Lines {start+1}-{end}:")
                        for j in range(start, end):
                            prefix = ">>> " if j == i else "    "
                            print(f"{prefix}{lines[j]}")
                        print()
            else:
                print("\nâœ— WanAttentionBlock does not use flash_attention")
                
        except Exception as e:
            print(f"Cannot analyze WanAttentionBlock: {e}")
    
    except Exception as e:
        print(f"Source analysis error: {e}")

def check_actual_flash_usage():
    """å®Ÿéš›ã«Flash AttentionãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª"""
    
    print("\nFlash Attentionä½¿ç”¨ç¢ºèª")
    print("="*30)
    
    # ãƒ•ãƒ©ã‚°ã‚’è¨­å®šã—ã¦Flash Attentionã®å‘¼ã³å‡ºã—ã‚’ç›£è¦–
    flash_call_count = 0
    standard_call_count = 0
    
    try:
        from flash_attn import flash_attn_func
        
        # flash_attn_funcã«ãƒ•ãƒƒã‚¯ã‚’è¿½åŠ 
        original_flash_attn_func = flash_attn_func
        
        def hooked_flash_attn_func(*args, **kwargs):
            nonlocal flash_call_count
            flash_call_count += 1
            print(f"ğŸš€ flash_attn_func called #{flash_call_count}")
            return original_flash_attn_func(*args, **kwargs)
        
        # ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒ
        import flash_attn
        flash_attn.flash_attn_func = hooked_flash_attn_func
        
        print("âœ“ Hooked flash_attn_func for monitoring")
        
    except ImportError:
        print("âœ— flash_attn not available for hooking")
    
    # æ¨™æº–ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚‚ãƒ•ãƒƒã‚¯
    try:
        original_sdpa = torch.nn.functional.scaled_dot_product_attention
        
        def hooked_sdpa(*args, **kwargs):
            nonlocal standard_call_count
            standard_call_count += 1
            if standard_call_count <= 5:  # æœ€åˆã®5å›ã ã‘è¡¨ç¤º
                print(f"ğŸ“Š scaled_dot_product_attention called #{standard_call_count}")
            return original_sdpa(*args, **kwargs)
        
        torch.nn.functional.scaled_dot_product_attention = hooked_sdpa
        print("âœ“ Hooked scaled_dot_product_attention for monitoring")
        
    except:
        print("âœ— Cannot hook scaled_dot_product_attention")
    
    return flash_call_count, standard_call_count

if __name__ == "__main__":
    test_wan_model_attention()
    analyze_wan_source()
    check_actual_flash_usage()
    
    print("\n" + "="*50)
    print("èª¿æŸ»å®Œäº† - å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œã§ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å‡ºåŠ›ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    print("="*50)